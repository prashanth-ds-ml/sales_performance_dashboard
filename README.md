
---

```markdown
# Superstore Sales Performance Dashboard (SQL + Streamlit + SQLite)

Interactive sales analytics on the classic **Superstore** dataset.  
Backend is **pure SQL on SQLite**, frontend is **Streamlit** with Altair/Plotly/Matplotlib.

- **Dataset**: https://www.kaggle.com/datasets/vivek468/superstore-dataset-final

---

## âœ¨ Whatâ€™s inside

- **SQLite-first pipeline** â€” build `superstore.db` from the cleaned CSV.
- **Saved SQL library** (`app/sql_queries.py`) â€” one place for all analysis queries.
- **Filter-aware dashboard** â€” sidebar filters (date, segment, region, category) are injected into saved queries via a **CTE wrapper** (no query duplication).
- **Zero-secrets deploy** â€” works on Streamlit Cloud without env vars.

---

## ğŸ“ Project Structure

```

sales\_performance\_dashboard/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ dashboard.py          # Streamlit app (uses saved queries only)
â”‚   â””â”€â”€ sql\_queries.py        # All SQL used by the app (SQLite syntax)
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ create\_sqlite\_db.py   # One-off script: CSV â†’ SQLite database
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Sample - Superstore.csv   # Raw dataset (optional)
â”‚   â”œâ”€â”€ superstore\_clean.csv      # Cleaned dataset (used to build DB)
â”‚   â””â”€â”€ superstore.db             # SQLite DB (recommended to commit)
â”‚
â”œâ”€â”€ queries/
â”‚   â””â”€â”€ sql\_eda.sql           # Original EDA/profiling (MySQL-style; kept for reference)
â”‚
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ config.toml           # (optional) theme/server config
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ runtime.txt               # (optional) pin Python for Cloud
â””â”€â”€ README.md

````

---

## ğŸš€ Quickstart

### 1) Create a virtual env & install deps
```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\Activate.ps1
pip install -r requirements.txt
````

### 2) Build the SQLite DB (from cleaned CSV)

```bash
python scripts/create_sqlite_db.py
# creates data/superstore.db
```

> The script normalizes column names, coerces types, and builds helpful indexes.

### 3) Run the dashboard locally

```bash
streamlit run app/dashboard.py
```

Open the URL printed in the terminal.

---

## â˜ï¸ Deploy to Streamlit Cloud (public)

1. Push this repo (including `data/superstore.db` **or** `data/superstore_clean.csv`).
2. In Streamlit Cloud: **New app** â†’ select repo/branch â†’ main file: `app/dashboard.py`.
3. Deploy.

   * If `superstore.db` is present â†’ instant startup.
   * If only `superstore_clean.csv` is present â†’ the app auto-builds the DB on first run.

**Recommended pinned versions** (already in `requirements.txt`):

```
streamlit==1.37.1
pandas==2.2.2
numpy==1.26.4
altair==5.3.0
plotly==5.23.0
matplotlib==3.8.4
```

**Optional**:

* `runtime.txt` â†’ `python-3.11.9`
* `.streamlit/config.toml` for theme:

  ```toml
  [server]
  headless = true
  runOnSave = true

  [theme]
  base = "dark"
  ```

---

## ğŸ§± How the dashboard works

* All charts/tables are driven by **saved SQL** in `app/sql_queries.py` (e.g., `MONTHLY_SALES_PROFIT`, `SEGMENT_RPM`, `REGION_PERF`, â€¦).
* Sidebar filters build a `WHERE` clause and the app injects it with a **safe CTE wrapper**:

  ```sql
  WITH filtered_sales AS (
    SELECT * FROM sales
    WHERE date(order_date) BETWEEN :s AND :e
      AND segment IN (...)
      AND region  IN (...)
      AND category IN (...)
  )
  -- Your saved query runs with 'FROM sales' rewritten to 'FROM filtered_sales'
  ```
* This keeps queries reusable and filter-aware without editing each SQL.

---

## ğŸ“Š What youâ€™ll see in the app

* **KPIs**: Total Sales, Profit, Orders, Avg Discount
* **Time Series**: Monthly Sales & Profit, optional MoM %
* **Segments & Categories**: performance bars + tables
* **Geography**: Region performance, Top States, Loss-making states
* **Products**: Top by revenue/quantity, High sales but â‰¤0 profit
* **Shipping & Discounts**: Avg ship days by region, monthly avg discount, discountâ†”profit scatter

Each table comes with a **Download CSV** button.

---

## ğŸ§ª Data prep (source of truth)

* Clean the raw CSV in `notebooks/data.ipynb` (or your preferred process).
  Typical steps:

  * normalize headers â†’ snake\_case
  * trim strings, fix types
  * parse `order_date`, `ship_date` to `YYYY-MM-DD`
  * export **`data/superstore_clean.csv`**
* Build the DB with `scripts/create_sqlite_db.py`.

> We keep `queries/sql_eda.sql` (MySQL-style) for reference, but the app uses the **SQLite** equivalents in `app/sql_queries.py`.

---

## ğŸ›  Troubleshooting

* **â€œDB not foundâ€**
  Ensure either `data/superstore.db` (recommended) or `data/superstore_clean.csv` (auto-build) is present.
* **Path issues on Cloud**
  Paths in `dashboard.py` are relative to the file: `../data/superstore.db`.
* **Window functions**
  The MoM query uses `LAG()`. Streamlit Cloudâ€™s SQLite supports it; if your env doesnâ€™t, replace with a self-join version.
* **Module conflicts**
  Donâ€™t keep another `app/app.py` that imports MySQL secrets. Keep the Streamlit app at `app/dashboard.py` and SQL at `app/sql_queries.py`.

---

## ğŸ§© Extending the app

* Add/modify queries in `app/sql_queries.py` and reference them in `dashboard.py`.
* Create new tabs/sections with your queries (the filter wrapper will apply automatically).
* For heavy workloads, consider **pre-aggregated** tables or switching to **DuckDB**.

---

## ğŸ¤ Contributing

PRs welcome! Please:

1. Keep SQL in `app/sql_queries.py` (SQLite syntax).
2. Add visuals in `app/dashboard.py` using the CTE wrapper.
3. Pin any new Python deps in `requirements.txt`.

---

