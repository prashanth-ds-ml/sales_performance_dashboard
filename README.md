---

# Superstore Sales Performance Dashboard (SQL + Streamlit + SQLite)

Interactive sales analytics on the classic **Superstore** dataset.
Backend is **pure SQL on SQLite**, frontend is **Streamlit** with Altair/Plotly/Matplotlib.

Dataset: [https://www.kaggle.com/datasets/vivek468/superstore-dataset-final](https://www.kaggle.com/datasets/vivek468/superstore-dataset-final)

---

##  What‚Äôs inside

* **SQLite-first pipeline** ‚Äî build `superstore.db` from the cleaned CSV.
* **Saved SQL library** (`app/sql_queries.py`) ‚Äî one place for all analysis queries.
* **Filter-aware dashboard** ‚Äî sidebar filters (date, segment, region, category) are injected into saved queries via a **CTE wrapper** (no query duplication).
* **Zero-secrets deploy** ‚Äî works on Streamlit Cloud without env vars.

---

##  Project Structure

```
sales_performance_dashboard/
‚îÇ
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py          # Streamlit app (uses saved queries only)
‚îÇ   ‚îî‚îÄ‚îÄ sql_queries.py        # All SQL used by the app (SQLite syntax)
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ create_sqlite_db.py   # One-off script: CSV ‚Üí SQLite database
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ Sample - Superstore.csv   # Raw dataset (optional)
‚îÇ   ‚îú‚îÄ‚îÄ superstore_clean.csv      # Cleaned dataset (used to build DB)
‚îÇ   ‚îî‚îÄ‚îÄ superstore.db             # SQLite DB (recommended to commit)
‚îÇ
‚îú‚îÄ‚îÄ queries/
‚îÇ   ‚îî‚îÄ‚îÄ sql_eda.sql           # Original EDA/profiling (MySQL-style; kept for reference)
‚îÇ
‚îú‚îÄ‚îÄ .streamlit/
‚îÇ   ‚îî‚îÄ‚îÄ config.toml           # (optional) theme/server config
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ runtime.txt               # (optional) pin Python for Cloud
‚îî‚îÄ‚îÄ README.md
```

---

##  Quickstart

### 1) Create a virtual env & install deps

```bash
python -m venv .venv
source .venv/bin/activate      # Windows: .venv\Scripts\Activate.ps1
pip install -r requirements.txt
```

### 2) Build the SQLite DB (from cleaned CSV)

```bash
python scripts/create_sqlite_db.py
# creates data/superstore.db
```

> The script normalizes column names, coerces types, and builds helpful indexes.

### 3) Run the dashboard locally

```bash
streamlit run app/dashboard.py
```

Open the URL printed in the terminal.

---

##  Deploy to Streamlit Cloud (public)

1. Push this repo (including `data/superstore.db` **or** `data/superstore_clean.csv`).
2. In Streamlit Cloud: **New app** ‚Üí select repo/branch ‚Üí main file: `app/dashboard.py`.
3. Deploy.

   * If `superstore.db` is present ‚Üí instant startup.
   * If only `superstore_clean.csv` is present ‚Üí the app auto-builds the DB on first run.

**Recommended pinned versions** (already in `requirements.txt`):

```
streamlit==1.37.1
pandas==2.2.2
numpy==1.26.4
altair==5.3.0
plotly==5.23.0
matplotlib==3.8.4
```

**Optional**:

* `runtime.txt` ‚Üí `python-3.11.9`
* `.streamlit/config.toml` for theme:

  ```toml
  [server]
  headless = true
  runOnSave = true

  [theme]
  base = "dark"
  ```

---

##  How the dashboard works

* All charts/tables are driven by **saved SQL** in `app/sql_queries.py` (e.g., `MONTHLY_SALES_PROFIT`, `SEGMENT_RPM`, `REGION_PERF`, ‚Ä¶).
* Sidebar filters build a `WHERE` clause and the app injects it with a **safe CTE wrapper**:

  ```sql
  WITH filtered_sales AS (
    SELECT * FROM sales
    WHERE date(order_date) BETWEEN :s AND :e
      AND segment IN (...)
      AND region  IN (...)
      AND category IN (...)
  )
  -- Your saved query runs with 'FROM sales' rewritten to 'FROM filtered_sales'
  ```
* This keeps queries reusable and filter-aware without editing each SQL.

---

##  What you‚Äôll see in the app

* **KPIs**: Total Sales, Profit, Orders, Avg Discount
* **Time Series**: Monthly Sales & Profit, optional MoM %
* **Segments & Categories**: performance bars + tables
* **Geography**: Region performance, Top States, Loss-making states
* **Products**: Top by revenue/quantity, High sales but ‚â§0 profit
* **Shipping & Discounts**: Avg ship days by region, monthly avg discount, discount‚Üîprofit scatter

Each table comes with a **Download CSV** button.

---

##  Data prep (source of truth)

* Clean the raw CSV in `notebooks/data.ipynb` (or your preferred process).
  Typical steps:

  * normalize headers ‚Üí snake\_case
  * trim strings, fix types
  * parse `order_date`, `ship_date` to `YYYY-MM-DD`
  * export **`data/superstore_clean.csv`**
* Build the DB with `scripts/create_sqlite_db.py`.

> We keep `queries/sql_eda.sql` (MySQL-style) for reference, but the app uses the **SQLite** equivalents in `app/sql_queries.py`.

---

##  Troubleshooting

* **‚ÄúDB not found‚Äù**
  Ensure either `data/superstore.db` (recommended) or `data/superstore_clean.csv` (auto-build) is present.
* **Path issues on Cloud**
  Paths in `dashboard.py` are relative to the file: `../data/superstore.db`.
* **Window functions**
  The MoM query uses `LAG()`. Streamlit Cloud‚Äôs SQLite supports it; if your env doesn‚Äôt, replace with a self-join version (ask in Issues).
* **Module conflicts**
  Don‚Äôt keep another `app/app.py` that imports MySQL secrets. Keep the Streamlit app at `app/dashboard.py` and SQL at `app/sql_queries.py`.

---

##  Extending the app

* Add/modify queries in `app/sql_queries.py` and reference them in `dashboard.py`.
* Create new tabs/sections with your queries (the filter wrapper will apply automatically).
* For heavy workloads, consider **pre-aggregated** tables or switching to **DuckDB**.

---

##  Contributing

PRs welcome! Please:

1. Keep SQL in `app/sql_queries.py` (SQLite syntax).
2. Add visuals in `app/dashboard.py` using the CTE wrapper.
3. Pin any new Python deps in `requirements.txt`.

---

## üìú License

MIT (or your preferred license).

---

